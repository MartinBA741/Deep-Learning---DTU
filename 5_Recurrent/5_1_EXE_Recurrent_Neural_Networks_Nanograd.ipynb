{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 5 - Recurrent Neural Networks\r\n",
    "\r\n",
    "In this lab, we will introduce different ways of learning from sequential data.\r\n",
    "\r\n",
    "As a recurring example, we will train neural networks to do language modelling, i.e. predict the next token in a sentence. In the context of natural language processing a token could be a character or a word, but mind you that the concepts introduced here apply to all kinds of sequential data, such as e.g. protein sequences, weather measurements, audio signals, or videos, just to name a few.\r\n",
    "\r\n",
    "To really get a grasp of what is going on inside a recurrent neural network (RNN), we will carry out a substantial part of this exercise in Nanograd rather than PyTorch. \r\n",
    "\r\n",
    "We start off with a simple toy problem, build an RNN using Nanograd, train it, and see for ourselves that it really works. Once we're convinced, you will implement the Long Short-Term Memory (LSTM) cell, also in Nanograd. \r\n",
    "\r\n",
    "This is *not* simple but with the DenseLayer class we already have, it is doable. Having done it yourself will help you understand what happens under the hood of the PyTorch code we will use throughout the course.\r\n",
    "\r\n",
    "To summarize, in this notebook we will show you:\r\n",
    "* How to represent sequences of categorical variables\r\n",
    "* How to build and train an RNN in Nanograd\r\n",
    "* How to build and train an LSTM network in Nanograd\r\n",
    "* How to build and train an LSTM network in PyTorch\r\n",
    "\r\n",
    "\r\n",
    "[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)"
   ],
   "metadata": {
    "id": "y-CptVs7iACc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representing tokens or text\n",
    "\n",
    "In previous labs we mainly considered data $x \\in \\mathbb{R}^d$, where $d$ is the feature space dimension.\n",
    "With time sequences our data can be represented as $x \\in \\mathbb{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
    "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
    "\n",
    "With RNNs, we can model both many-to-one functions: $\\mathbb{R}^{t \\, \\times \\, d} \\rightarrow \\mathbb{R}^c$ and many-to-many functions: $\\mathbb{R}^{t \\, \\times \\, d} \\rightarrow \\mathbb{R}^{t \\, \\times \\, c}$, where $c$ is the amount of classes/output dimensions.\n",
    "\n",
    "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
    "\n",
    "In this exercise we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language) this may be infeasible. For such scenarios, you can project the encodings into a smaller space by use of embeddings. If you want to learn more about tokens, encodings and embeddings than what is covered in this exercise, we highly recommend [this lecture](https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)."
   ],
   "metadata": {
    "id": "XapO8SLwiACd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-hot encoding over vocabulary\n",
    "\n",
    "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
    "\n",
    "| vocabulary    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "\n",
    "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
    "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
    "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
    "\n",
    "Consider the following text\n",
    "> I love the corny jokes in Spielberg's new movie.\n",
    "\n",
    "where an example result would be similar to\n",
    "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
   ],
   "metadata": {
    "id": "bdA4LPsFiACe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating a dataset"
   ],
   "metadata": {
    "id": "KNmyPw7zk2gY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this exercise we will create a simple dataset that we can learn from. We generate sequences of the form:\n",
    "\n",
    "`a b EOS`,\n",
    "\n",
    "`a a b b EOS`,\n",
    "\n",
    "`a a a a a b b b b b EOS`\n",
    "\n",
    "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will follow 5 `a`s."
   ],
   "metadata": {
    "id": "M9IEA4t2k2gb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# Set seed such that we always get the same dataset\r\n",
    "# (this is a good idea in general)\r\n",
    "np.random.seed(42)\r\n",
    "\r\n",
    "def generate_dataset(num_sequences=2**5):\r\n",
    "    \"\"\"\r\n",
    "    Generates a number of sequences as our dataset.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "     `num_sequences`: the number of sequences to be generated.\r\n",
    "     \r\n",
    "    Returns a list of sequences.\r\n",
    "    \"\"\"\r\n",
    "    samples = []\r\n",
    "    \r\n",
    "    for _ in range(num_sequences): \r\n",
    "        num_tokens = np.random.randint(1, 4)\r\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\r\n",
    "        samples.append(sample)\r\n",
    "        \r\n",
    "    return samples\r\n",
    "\r\n",
    "\r\n",
    "sequences = generate_dataset()\r\n",
    "\r\n",
    "print('A single sample from the generated dataset:')\r\n",
    "print(sequences[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "metadata": {
    "id": "dcoN-kb7k2gc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representing tokens as indices"
   ],
   "metadata": {
    "id": "YMLd3Gzak2gp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `vocab_size` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
   ],
   "metadata": {
    "id": "S9LSqaJSk2gp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise a) Sequence to dictionary function \n",
    "\n",
    "Complete the sequences_to_dicts function below. You will need to fill the word_to_idx and idx_to_word dictionaries so that we can go back and forth between the two representations."
   ],
   "metadata": {
    "id": "sNY1OOS_k2gy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from collections import defaultdict\r\n",
    "\r\n",
    "def sequences_to_dicts(sequences):\r\n",
    "    \"\"\"\r\n",
    "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\r\n",
    "    \"\"\"\r\n",
    "    # A bit of Python-magic to flatten a nested list\r\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\r\n",
    "    \r\n",
    "    # Flatten the dataset\r\n",
    "    all_words = flatten(sequences)\r\n",
    "    \r\n",
    "    # Count number of word occurences\r\n",
    "    word_count = defaultdict(int)\r\n",
    "    for word in flatten(sequences):\r\n",
    "        word_count[word] += 1\r\n",
    "\r\n",
    "    # Sort by frequency\r\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\r\n",
    "\r\n",
    "    # Create a list of all unique words\r\n",
    "    unique_words = [item[0] for item in word_count]\r\n",
    "    \r\n",
    "    # Add UNK token to list of words\r\n",
    "    unique_words.append('UNK')\r\n",
    "\r\n",
    "    # Count number of sequences and number of unique words\r\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\r\n",
    "\r\n",
    "    # Create dictionaries so that we can go from word to index and back\r\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\r\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\r\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\r\n",
    "\r\n",
    "    # Fill dictionaries\r\n",
    "    for idx, word in enumerate(unique_words):\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        word_to_idx[word] = idx\r\n",
    "        idx_to_word[idx] = word\r\n",
    "\r\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\r\n",
    "\r\n",
    "\r\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\r\n",
    "\r\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\r\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\r\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\r\n",
    "\r\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\r\n",
    "    'Consistency error: something went wrong in the conversion.'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We have 32 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n"
     ]
    }
   ],
   "metadata": {
    "id": "Smdo70UMk2gr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partitioning the dataset"
   ],
   "metadata": {
    "id": "cGSoDRgHk2g1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To build our dataset, we need to create inputs and targets for each sequences and partition sentences it into training, validation and test sets. 80%, 10% and 10% is a common distribution, but mind you that this largely depends on the size of the dataset. Since we are doing next-word predictions, our target sequence is simply the input sequence shifted by one word.\r\n",
    "\r\n",
    "We can use PyTorch's `Dataset` class to build a simple dataset where we can easily retrieve (inputs, targets) pairs for each of our sequences."
   ],
   "metadata": {
    "id": "UMTn1iLIk2g1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch.utils import data\r\n",
    "\r\n",
    "class Dataset(data.Dataset):\r\n",
    "    def __init__(self, inputs, targets):\r\n",
    "        self.inputs = inputs\r\n",
    "        self.targets = targets\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        # Return the size of the dataset\r\n",
    "        return len(self.targets)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        # Retrieve inputs and targets at the given index\r\n",
    "        X = self.inputs[index]\r\n",
    "        y = self.targets[index]\r\n",
    "\r\n",
    "        return X, y\r\n",
    "\r\n",
    "    \r\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\r\n",
    "    # Define partition sizes\r\n",
    "    num_train = int(len(sequences)*p_train)\r\n",
    "    num_val = int(len(sequences)*p_val)\r\n",
    "    num_test = int(len(sequences)*p_test)\r\n",
    "\r\n",
    "    # Split sequences into partitions\r\n",
    "    sequences_train = sequences[:num_train]\r\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\r\n",
    "    sequences_test = sequences[-num_test:]\r\n",
    "\r\n",
    "    def get_inputs_targets_from_sequences(sequences):\r\n",
    "        # Define empty lists\r\n",
    "        inputs, targets = [], []\r\n",
    "        \r\n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\r\n",
    "        # but targets are shifted right by one so that we can predict the next word\r\n",
    "        for sequence in sequences:\r\n",
    "            inputs.append(sequence[:-1])\r\n",
    "            targets.append(sequence[1:])\r\n",
    "            \r\n",
    "        return inputs, targets\r\n",
    "\r\n",
    "    # Get inputs and targets for each partition\r\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\r\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\r\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\r\n",
    "\r\n",
    "    # Create datasets\r\n",
    "    training_set = dataset_class(inputs_train, targets_train)\r\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\r\n",
    "    test_set = dataset_class(inputs_test, targets_test)\r\n",
    "\r\n",
    "    return training_set, validation_set, test_set\r\n",
    "    \r\n",
    "\r\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\r\n",
    "\r\n",
    "print(f'We have {len(training_set)} samples in the training set.')\r\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\r\n",
    "print(f'We have {len(test_set)} samples in the test set.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We have 25 samples in the training set.\n",
      "We have 3 samples in the validation set.\n",
      "We have 3 samples in the test set.\n"
     ]
    }
   ],
   "metadata": {
    "id": "9dW7MrPnk2g3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When working with more complex data than what we use in this exercise, creating a PyTorch `DataLoader` on top of the dataset can be beneficial. A data loader is basically a fancy generator/iterator that we can use to abstract away all of the data handling and pre-processing + it's super useful for processing batches of data as well! Data loaders will come in handy later when you start to work on your projects, so be sure to check them out!\n",
    "\n",
    "For more information on how to use datasets and data loaders in PyTorch, [consult the official guide](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ],
   "metadata": {
    "id": "4xMMSm7Mk2g9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nanograd utilities"
   ],
   "metadata": {
    "id": "t-rfgDfZeMQ6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load necessary utility functions for the Nanograd library, which we saw in Lab 2."
   ],
   "metadata": {
    "id": "oRO5ssg0eQMK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\r\n",
    "\r\n",
    "from math import exp, log, tanh\r\n",
    "\r\n",
    "class Var:\r\n",
    "    \"\"\"\r\n",
    "    A variable which holds a float and enables gradient computations.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\r\n",
    "        assert type(val) == float\r\n",
    "        self.v = val\r\n",
    "        self.grad_fn = grad_fn\r\n",
    "        self.grad = 0.0\r\n",
    "\r\n",
    "    def backprop(self, bp):\r\n",
    "        self.grad += bp\r\n",
    "        for input, grad in self.grad_fn():\r\n",
    "            input.backprop(grad * bp)\r\n",
    "\r\n",
    "    def backward(self):\r\n",
    "        assert ~isinstance(self.grad_fn, list), 'grad_fn should be a list'\r\n",
    "        self.backprop(1.0)\r\n",
    "\r\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\r\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\r\n",
    "\r\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\r\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\r\n",
    "\r\n",
    "    def __pow__(self, power):\r\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\r\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\r\n",
    "\r\n",
    "    def __neg__(self: 'Var') -> 'Var':\r\n",
    "        return Var(-1.0) * self\r\n",
    "\r\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\r\n",
    "        return self + (-other)\r\n",
    "\r\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\r\n",
    "        return self * other ** -1\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\r\n",
    "    \r\n",
    "    def exp(self):\r\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\r\n",
    "    \r\n",
    "    def log(self):\r\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])\r\n",
    "\r\n",
    "    def relu(self):\r\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\r\n",
    "    \r\n",
    "    def identity(self):\r\n",
    "        return self\r\n",
    "\r\n",
    "    def sigmoid(self):\r\n",
    "        return Var(0.5) * (Var(1.0) + (Var(0.5) * self).tanh()) # logistic function is a scaled and shifted version of tanh\r\n",
    "    \r\n",
    "    def tanh(self):\r\n",
    "        return Var(tanh(self.v), lambda: [(self, 1-tanh(self.v) ** 2)])"
   ],
   "outputs": [],
   "metadata": {
    "id": "Jd4CoEBNzNWS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# convert from ndarray to Var\r\n",
    "def nparray_to_Var(x):\r\n",
    "  if x.ndim==1:\r\n",
    "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\r\n",
    "  else:\r\n",
    "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\r\n",
    "  return y\r\n",
    "\r\n",
    "# convert from Var to ndarray  \r\n",
    "def Var_to_nparray(x):\r\n",
    "  try:\r\n",
    "    y = np.zeros((len(x),len(x[0])))\r\n",
    "    for i in range(len(x)):\r\n",
    "      for j in range(len(x[0])):\r\n",
    "        y[i,j] = x[i][j].v\r\n",
    "  except TypeError:\r\n",
    "    y = np.zeros((len(x)))\r\n",
    "    for i in range(len(x)):\r\n",
    "      y[i] = x[i].v\r\n",
    "\r\n",
    "  return y"
   ],
   "outputs": [],
   "metadata": {
    "id": "9AMqMsiseMfz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Initializer:\r\n",
    "\r\n",
    "  def init_weights(self, n_in, n_out):\r\n",
    "    raise NotImplementedError\r\n",
    "\r\n",
    "  def init_bias(self, n_out):\r\n",
    "    raise NotImplementedError"
   ],
   "outputs": [],
   "metadata": {
    "id": "ij_ieRsAt7Xt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import random\r\n",
    "\r\n",
    "class NormalInitializer(Initializer):\r\n",
    "\r\n",
    "  def __init__(self, mean=0, std=0.1):\r\n",
    "    self.mean = mean\r\n",
    "    self.std = std\r\n",
    "\r\n",
    "  def init_weights(self, n_in, n_out):\r\n",
    "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\r\n",
    "\r\n",
    "  def init_bias(self, n_out):\r\n",
    "    return [Var(0.0) for _ in range(n_out)]\r\n",
    "\r\n",
    "class ConstantInitializer(Initializer):\r\n",
    "\r\n",
    "  def __init__(self, weight=1.0, bias=0.0):\r\n",
    "    self.weight = weight\r\n",
    "    self.bias = bias\r\n",
    "\r\n",
    "  def init_weights(self, n_in, n_out):\r\n",
    "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\r\n",
    "\r\n",
    "  def init_bias(self, n_out):\r\n",
    "    return [Var(self.bias) for _ in range(n_out)]"
   ],
   "outputs": [],
   "metadata": {
    "id": "eb18N5phuIha"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-hot encodings"
   ],
   "metadata": {
    "id": "Dzmryk72k2g-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the entire vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence."
   ],
   "metadata": {
    "id": "abRN9f8Xk2g_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def one_hot_encode(idx, vocab_size):\r\n",
    "    \"\"\"\r\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "     `idx`: the index of the given word\r\n",
    "     `vocab_size`: the size of the vocabulary\r\n",
    "    \r\n",
    "    Returns a 1-D numpy array of length `vocab_size`.\r\n",
    "    \"\"\"\r\n",
    "    # Initialize the encoded array\r\n",
    "    one_hot = np.array([np.zeros(vocab_size)])\r\n",
    "    \r\n",
    "    # Set the appropriate element to one\r\n",
    "    one_hot[0][idx] = 1.0\r\n",
    "    return nparray_to_Var(one_hot)\r\n",
    "\r\n",
    "\r\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\r\n",
    "    \"\"\"\r\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "     `sentence`: a list of words to encode\r\n",
    "     `vocab_size`: the size of the vocabulary\r\n",
    "     \r\n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\r\n",
    "    \"\"\"\r\n",
    "    # Encode each word in the sentence\r\n",
    "    encoding = np.array([Var_to_nparray(one_hot_encode(word_to_idx[word], vocab_size)) for word in sequence])\r\n",
    "\r\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\r\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[2], 1)\r\n",
    "    return nparray_to_Var(encoding)\r\n",
    "\r\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\r\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {Var_to_nparray(test_word).shape}.')\r\n",
    "\r\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\r\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {Var_to_nparray(test_sentence).shape}.')\r\n",
    "\r\n",
    "print(test_word)\r\n",
    "print(test_sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our one-hot encoding of 'a' has shape (1, 4).\n",
      "Our one-hot encoding of 'a b' has shape (2, 4).\n",
      "[[Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]]\n",
      "[[Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000), Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]]\n"
     ]
    }
   ],
   "metadata": {
    "id": "IZruCIHJk2hB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "encoded_training_set_x = []\r\n",
    "encoded_training_set_y = []\r\n",
    "encoded_validation_set_x = []\r\n",
    "encoded_validation_set_y = []\r\n",
    "encoded_test_set_x = []\r\n",
    "encoded_test_set_y = []\r\n",
    "\r\n",
    "for n in range(len(training_set)):\r\n",
    "  encoded_training_set_x.append(one_hot_encode_sequence(training_set[n][0], vocab_size))\r\n",
    "  encoded_training_set_y.append(one_hot_encode_sequence(training_set[n][1], vocab_size))\r\n",
    "for n in range(len(validation_set)):\r\n",
    "  encoded_validation_set_x.append(one_hot_encode_sequence(validation_set[n][0], vocab_size))\r\n",
    "  encoded_validation_set_y.append(one_hot_encode_sequence(validation_set[n][1], vocab_size))\r\n",
    "for n in range(len(test_set)):\r\n",
    "  encoded_test_set_x.append(one_hot_encode_sequence(test_set[n][0], vocab_size))\r\n",
    "  encoded_test_set_y.append(one_hot_encode_sequence(test_set[n][1], vocab_size))"
   ],
   "outputs": [],
   "metadata": {
    "id": "JT6BqYrU_NxQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Now that we have our one-hot encodings in place, we can move on to the RNNs!"
   ],
   "metadata": {
    "id": "erI_MXvKk2hG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Recurrent Neural Networks (RNN)\r\n",
    "\r\n",
    "Reading material: [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and (optionally) [this lecture](https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).\r\n",
    "\r\n",
    "___\r\n",
    "\r\n",
    "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\r\n",
    "\r\n",
    "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\r\n",
    "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\r\n",
    "An image may best explain how this is to be understood,\r\n",
    "\r\n",
    "![rnn-unroll image](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/rnn-unfold.png?raw=1)\r\n",
    "\r\n",
    "\r\n",
    "where it the network contains the following elements:\r\n",
    "\r\n",
    "- $x$ is the input sequence of samples, \r\n",
    "- $U$ is a weight matrix applied to the given input sample,\r\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\r\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\r\n",
    "- $h$ is the hidden state (the network's memory) for a given time step, and\r\n",
    "- $o$ is the resulting output.\r\n",
    "\r\n",
    "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\r\n",
    "We have the following computations through the network:\r\n",
    "\r\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\r\n",
    "- $o_t = W\\,{h_t}$\r\n",
    "\r\n",
    "When we are doing language modelling using a cross-entropy loss, we additionally apply the softmax function to the output $o_{t}$:\r\n",
    "\r\n",
    "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\r\n",
    "\r\n",
    "\r\n",
    "### Backpropagation through time\r\n",
    "\r\n",
    "We define a loss function\r\n",
    "\r\n",
    "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\r\n",
    "\r\n",
    "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function.\r\n",
    "\r\n",
    "Backpropagation through time amounts to computing the gradients of the loss using the same type of clever bookkeeping we applied to the feed-forward network in week 1. This you will do in Exercise D."
   ],
   "metadata": {
    "id": "MA6bxjGWjeSB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing an RNN\r\n",
    "\r\n",
    "We will implement the forward pass, backward pass, optimization and training loop for an RNN in Nanograd so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch."
   ],
   "metadata": {
    "id": "GuvwbvsGz9KE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the Nanograd DenseLayer class from [lab 2](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_Python/2.1-EXE-FNN-AutoDif-Nanograd.ipynb) with a few additions:\r\n",
    "* the option use_bias to define a layer without bias. This is useful when we define the recurrent layer and\r\n",
    "* a method forward_sequence which is useful when a DenseLayer is used as part of a recurrent neural network"
   ],
   "metadata": {
    "id": "gfbfcB-NJZuM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "from typing import Sequence\r\n",
    "\r\n",
    "class DenseLayer:\r\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer(), use_bias=True):\r\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\r\n",
    "        self.use_bias = use_bias\r\n",
    "        if use_bias:\r\n",
    "          self.bias = initializer.init_bias(n_out)\r\n",
    "        self.act_fn = act_fn\r\n",
    "    \r\n",
    "    def __repr__(self):    \r\n",
    "        return 'Weights: ' + repr(self.weights) + ('\\n Biases: ' + repr(self.bias) if self.use_bias else '')\r\n",
    "\r\n",
    "    def parameters(self) -> Sequence[Var]:\r\n",
    "      params = []\r\n",
    "      for r in self.weights:\r\n",
    "        params += r\r\n",
    "\r\n",
    "      if self.use_bias:\r\n",
    "        params += self.bias\r\n",
    "\r\n",
    "      return params\r\n",
    "\r\n",
    "    def forward(self, input: Sequence[Var]) -> Sequence[Var]:\r\n",
    "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \r\n",
    "        # to the current layer matches the number of nodes in the current layer\r\n",
    "        assert len(self.weights) == len(input), \"weights and input must match in first dimension\"\r\n",
    "        weights = self.weights\r\n",
    "        out = []\r\n",
    "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\r\n",
    "        # We therefore loop over the (number of) nodes in the current layer:\r\n",
    "        for j in range(len(weights[0])): \r\n",
    "            # Initialize the node value depending on its corresponding parameters.\r\n",
    "            node = self.bias[j] if self.use_bias else Var(0.0)\r\n",
    "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\r\n",
    "            for i in range(len(input)):\r\n",
    "                node += input[i]*weights[i][j]\r\n",
    "            node = self.act_fn(node)\r\n",
    "            out.append(node)\r\n",
    "\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]]) -> Sequence[Sequence[Var]]:\r\n",
    "        out = []\r\n",
    "        for i in range(len(input)): \r\n",
    "            node = self.forward(input[i])\r\n",
    "            out.append(node)\r\n",
    "\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {
    "id": "TqkVyEEACHKS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise b) The RNNLayer class\r\n",
    "\r\n",
    "Complete the RNNLayer class below.\r\n",
    "\r\n",
    "Explain how we reuse the DenseLayer class.\r\n",
    "\r\n",
    "Explain what the forward and the forward_sequence method do."
   ],
   "metadata": {
    "id": "qDKFjjQEM-xX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer b)\r\n",
    "* We reuse the DenseLayer class in the RNN to keep track of all the dimensions and parameters of our RNN. \\\r\n",
    "Moreover, the DenseLayer class has the two build-in methods:\r\n",
    "    - The forward method, which apply the non-linear activation function to compute the value of each node. This is done in a loop over each node in a layer. \r\n",
    "    - The forward_sequence applies the forward method and loop over each layer in a network. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from typing import Sequence\r\n",
    "\r\n",
    "class RNNLayer:\r\n",
    "    def __init__(self, n_in: int, n_hid: int, act_fn, initializer = NormalInitializer(), initializer_hid = NormalInitializer()):\r\n",
    "        self.n_hid = n_hid\r\n",
    "        self.in_hid_layer = DenseLayer(n_in, n_hid , lambda x: x, initializer)\r\n",
    "        self.hid_hid_layer = DenseLayer (n_in, n_hid, lambda x: x, initializer_hid, use_bias=False) # we already get a bias through in_hid_layer, therefore, use_bias=False\r\n",
    "        self.initial_hid = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.stored_hid = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.act_fn = act_fn\r\n",
    "    \r\n",
    "    def __repr__(self):    \r\n",
    "        return '\\n Feed-forward: ' + repr(self.in_hid_layer) + '\\n Recurrent: ' + repr(self.hid_hid_layer) + '\\n Initial hidden: ' + repr(self.initial_hid)\r\n",
    "\r\n",
    "    def parameters(self) -> Sequence[Var]:      \r\n",
    "      return self.in_hid_layer.parameters() + self.hid_hid_layer.parameters() + self.initial_hid\r\n",
    "\r\n",
    "    def forward_step(self, input: Sequence[Var], input_hid: Sequence[Var]) -> Sequence[Var]:\r\n",
    "        in_hids  = self.in_hid_layer.forward(input)         # contribution from input\r\n",
    "        hid_hids = self.hid_hid_layer.forward(input_hid)    # contribution from hidden state\r\n",
    "\r\n",
    "        hids = []\r\n",
    "        for i in range(self.n_hid):\r\n",
    "            hids.append(in_hids[i] + hid_hids[i-1])\r\n",
    "\r\n",
    "        return hids\r\n",
    "    \r\n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]], use_stored_hid = False) -> Sequence[Sequence[Var]]:\r\n",
    "        out = []\r\n",
    "        if use_stored_hid:\r\n",
    "            hid = self.stored_hid\r\n",
    "        else:\r\n",
    "            hid = self.initial_hid\r\n",
    "        \r\n",
    "        # Takes a sequence and loops over each character in the sequence. Note that each character has dimension equal to the embedding dimension\r\n",
    "        for i in range(len(input)):\r\n",
    "            hid = self.forward_step(input=input[0], input_hid=input[1])\r\n",
    "            out.append(hid)\r\n",
    "        self.stored_hid = hid\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {
    "id": "IcM1N6PQrT7l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can define a network and pass some data through it."
   ],
   "metadata": {
    "id": "VgAU6qPHKJFr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "NN = [\r\n",
    "    RNNLayer(1, 5, lambda x: x.tanh()),\r\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\r\n",
    "]\r\n",
    "\r\n",
    "def forward_batch(input: Sequence[Sequence[Sequence[Var]]], network, use_stored_hid=False):\r\n",
    "  \r\n",
    "  def forward_single_sequence(x, network, use_stored_hid):\r\n",
    "    for layer in network:\r\n",
    "        if isinstance(layer, RNNLayer):\r\n",
    "            x = layer.forward_sequence(x, use_stored_hid) \r\n",
    "        else:\r\n",
    "            x = layer.forward_sequence(x)\r\n",
    "    return x\r\n",
    "\r\n",
    "  output = [ forward_single_sequence(input[n], network, use_stored_hid) for n in range(len(input))]\r\n",
    "  return output\r\n",
    "\r\n",
    "print(NN[0])\r\n",
    "x_train =[\r\n",
    "          [[Var(1.0)], [Var(2.0)], [Var(3.0)]],\r\n",
    "          [[Var(1.0)], [Var(2.0)], [Var(3.0)]]\r\n",
    "          ]\r\n",
    "\r\n",
    "output_train = forward_batch(x_train, NN)          \r\n",
    "output_train[0][0][0].backward()\r\n",
    "\r\n",
    "print('\\n output_train: \\n', output_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Feed-forward: Weights: [[Var(v=-0.0392, grad=0.0000), Var(v=-0.0072, grad=0.0000), Var(v=-0.0570, grad=0.0000), Var(v=0.0123, grad=0.0000), Var(v=-0.1006, grad=0.0000)]]\n",
      " Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      " Recurrent: Weights: [[Var(v=0.0731, grad=0.0000), Var(v=-0.0296, grad=0.0000), Var(v=0.0198, grad=0.0000), Var(v=-0.0341, grad=0.0000), Var(v=0.0615, grad=0.0000)]]\n",
      " Initial hidden: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "\n",
      " output_train: \n",
      " [[[Var(v=-0.0017, grad=1.0000)], [Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)]], [[Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)]]]\n"
     ]
    }
   ],
   "metadata": {
    "id": "MFkZ5gNG6d7c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise c) Unit test\r\n",
    "\r\n",
    "Make unit tests to make sure that the output and the backward method work as it should.\r\n",
    "\r\n",
    "NOTE: The .backward() call above simply backpropagates a value in the output (and not a loss). Below, we will extend our loss functions to be able to handle backpropagation through time.\r\n",
    "\r\n",
    "Recycling code from [Lab 2](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_Python/2.1-EXE-FNN-AutoDif-Nanograd.ipynb) is fine. "
   ],
   "metadata": {
    "id": "yolo5dKrk2hR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# Insert code here\r\n",
    "# Added this to Class Var in the beginning:\r\n",
    "\r\n",
    "def backward(self):\r\n",
    "        assert ~isinstance(self.grad_fn, list), 'grad_fn should be a list'\r\n",
    "        self.backprop(1.0)\r\n",
    "\r\n",
    "# Check:\r\n",
    "isinstance(output_train[0][0][0].grad_fn(), list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {
    "id": "GhCB1ASwK3X7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise d) Advanced initialization\n",
    "\n",
    "How can we use He initialization for the recurrent layer?\n",
    "\n",
    "Hint: the sum of two unit variance stochastic variables have variance 2.\n",
    "\n",
    "Insert code for He initialization of the recurrent layer. Again, recycling code from Lab 2 is fine. "
   ],
   "metadata": {
    "id": "4d4_2b6mK5jH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "## He\r\n",
    "def DenseLayer_He_tanh(n_in: int, n_out: int):\r\n",
    "  std =  ((1)/(n_in))**(1/2) # <- replace with proper initialization \r\n",
    "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\r\n",
    "\r\n",
    "# Note that I have changed the labda function to x.tanh to follow the naming of the function. \r\n",
    "# Therefore, I have also used 1 in the numerator as were given in Lab 2:\r\n",
    "# \"For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$.\""
   ],
   "outputs": [],
   "metadata": {
    "id": "oRn3mDnzLxu2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise e) Sequence loss function\r\n",
    "\r\n",
    "We want to solve a sequence to sequence problem. So you need a sequence loss function. \r\n",
    "\r\n",
    "Implement the function such that the sequence loss can take flexible input dimensions and so that it can take any loss as an argument, such as squared loss and cross entropy. (We recommend using cross entropy below)\r\n",
    "\r\n",
    "We have provided a bit of code to try it out.\r\n",
    "\r\n",
    "Hints: You can get inspiration from the forward_sequence method above. You can copy and paste squared loss and cross entropy from Lab 2. "
   ],
   "metadata": {
    "id": "ozNN9xXML0yc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# Insert code here\r\n",
    "\r\n",
    "def squared_loss_sequence(t, y):\r\n",
    "    \r\n",
    "    # add check that sizes agree\r\n",
    "    assert len(t)==len(y)\r\n",
    "    \r\n",
    "    def squared_loss_single(t, y):\r\n",
    "\r\n",
    "        # add check that sizes agree\r\n",
    "        assert len(t)==len(y)\r\n",
    "        \r\n",
    "        Loss = Var(0.0)\r\n",
    "        for i in range(len(t)): # sum over outputs\r\n",
    "          for n in range(len(t[i])):\r\n",
    "            Loss += (t[i][n]-y[i][n]) ** 2.0\r\n",
    "        \r\n",
    "        return Loss\r\n",
    "\r\n",
    "    Loss = Var(0.0)\r\n",
    "    for n in range(len(t)): # sum over training data\r\n",
    "      Loss += squared_loss_single(t[n],y[n])\r\n",
    "    \r\n",
    "    return Loss\r\n",
    "\r\n",
    "\r\n",
    "def cross_entropy_loss_sequence(t, y):\r\n",
    "\r\n",
    "  # add check that sizes agree\r\n",
    "  assert len(t)==len(y)\r\n",
    "\r\n",
    "  def cross_entropy_loss_single(t, y):\r\n",
    "    assert len(t)==len(y), \"one-hot encodings have different shapes\"\r\n",
    "    Loss = Var(0.0)\r\n",
    "    denominator = Var(0.0)\r\n",
    "    for i in range(len(t)): # sum over one-hot-encoding of character\r\n",
    "        denominator += t[i].exp()\r\n",
    "    for i in range(len(t)): # sum over one-hot-encoding of character\r\n",
    "        t_softmax = t[i].exp() / denominator\r\n",
    "        Loss += -y[i]*(t_softmax+Var(0.001)).log()\r\n",
    "    return Loss\r\n",
    "\r\n",
    "  Loss = Var(0.0)\r\n",
    "  for n in range(len(t)): # sum over training data\r\n",
    "    for i in range(len(t[n])): # sum over characters in output sequence\r\n",
    "      Loss += cross_entropy_loss_single(t[n][i],y[n][i])\r\n",
    "  return Loss\r\n",
    "    \r\n",
    "\r\n",
    "def sequence_loss(t: Sequence[Sequence[Var]], y: Sequence[Sequence[Var]], loss_fn=squared_loss_sequence) -> Var:\r\n",
    "    assert len(t) == len(y)\r\n",
    "    return loss_fn(t, y)\r\n",
    "\r\n",
    "\r\n",
    "# Test of loss func\r\n",
    "NN = [\r\n",
    "    RNNLayer(4, 2, lambda x: x.tanh()),\r\n",
    "    DenseLayer(2, 4, lambda x: x.identity())\r\n",
    "]\r\n",
    "\r\n",
    "output_train = forward_batch(encoded_training_set_x[:3], NN)\r\n",
    "print(output_train)       \r\n",
    "loss = sequence_loss(output_train, encoded_training_set_y[:3], squared_loss_sequence)\r\n",
    "print(\"Loss:\", loss)\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "print(\"Output:\", output_train)\r\n",
    "\r\n",
    "print('Network before update:')\r\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \r\n",
    "\r\n",
    "def parameters(network):\r\n",
    "  params = []\r\n",
    "  for layer in range(len(network)):\r\n",
    "    params += network[layer].parameters()\r\n",
    "  return params\r\n",
    "\r\n",
    "def update_parameters(params, learning_rate=0.01):\r\n",
    "  for p in params:\r\n",
    "    p.v -= learning_rate*p.grad\r\n",
    "\r\n",
    "def zero_gradients(params):\r\n",
    "  for p in params:\r\n",
    "    p.grad = 0.0\r\n",
    "\r\n",
    "update_parameters(parameters(NN))\r\n",
    "\r\n",
    "print('\\nNetwork after update:')\r\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \r\n",
    "\r\n",
    "zero_gradients(parameters(NN))\r\n",
    "\r\n",
    "print('\\nNetwork after zeroing gradients:')\r\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)]], [[Var(v=0.0164, grad=0.0000), Var(v=0.0340, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=0.0041, grad=0.0000)], [Var(v=0.0164, grad=0.0000), Var(v=0.0340, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=0.0041, grad=0.0000)]], [[Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)], [Var(v=-0.0008, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=0.0043, grad=0.0000)]]]\n",
      "Loss: Var(v=13.8102, grad=0.0000)\n",
      "Output: [[[Var(v=-0.0008, grad=-2.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-2.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=-1.9991), Var(v=0.0043, grad=0.0086)]], [[Var(v=0.0164, grad=0.0328), Var(v=0.0340, grad=-1.9320), Var(v=0.0138, grad=0.0275), Var(v=0.0041, grad=0.0082)], [Var(v=0.0164, grad=0.0328), Var(v=0.0340, grad=0.0680), Var(v=0.0138, grad=-1.9725), Var(v=0.0041, grad=0.0082)]], [[Var(v=-0.0008, grad=-2.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-2.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=-1.9828), Var(v=0.0005, grad=0.0009), Var(v=0.0043, grad=0.0086)], [Var(v=-0.0008, grad=-0.0016), Var(v=0.0086, grad=0.0172), Var(v=0.0005, grad=-1.9991), Var(v=0.0043, grad=0.0086)]]]\n",
      "Network before update:\n",
      "Layer 0 \n",
      " \n",
      " Feed-forward: Weights: [[Var(v=-0.1082, grad=2.0140), Var(v=0.0163, grad=3.4809)], [Var(v=0.0744, grad=0.0000), Var(v=-0.0485, grad=0.0000)], [Var(v=0.1754, grad=0.0000), Var(v=0.0701, grad=0.0000)], [Var(v=0.0953, grad=0.0000), Var(v=-0.1642, grad=0.0000)]]\n",
      " Biases: [Var(v=0.0000, grad=2.0140), Var(v=0.0000, grad=3.4809)]\n",
      " Recurrent: Weights: [[Var(v=-0.0583, grad=2.9880), Var(v=0.1245, grad=1.7833)], [Var(v=-0.1387, grad=0.4929), Var(v=-0.0159, grad=0.2308)], [Var(v=-0.1524, grad=0.0000), Var(v=0.1099, grad=0.0000)], [Var(v=0.0207, grad=0.0000), Var(v=-0.3126, grad=0.0000)]]\n",
      " Initial hidden: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=-0.1098, grad=-0.1383), Var(v=-0.0516, grad=0.0400), Var(v=-0.0725, grad=0.1767), Var(v=0.0493, grad=-0.0004)], [Var(v=-0.0228, grad=0.3285), Var(v=-0.2254, grad=0.7229), Var(v=-0.0389, grad=0.4053), Var(v=-0.0834, grad=-0.0063)]]\n",
      " Biases: [Var(v=0.0000, grad=-7.9541), Var(v=0.0000, grad=-13.6572), Var(v=0.0000, grad=-5.9340), Var(v=0.0000, grad=0.1196)]\n",
      "\n",
      "Network after update:\n",
      "Layer 0 \n",
      " \n",
      " Feed-forward: Weights: [[Var(v=-0.1284, grad=2.0140), Var(v=-0.0185, grad=3.4809)], [Var(v=0.0744, grad=0.0000), Var(v=-0.0485, grad=0.0000)], [Var(v=0.1754, grad=0.0000), Var(v=0.0701, grad=0.0000)], [Var(v=0.0953, grad=0.0000), Var(v=-0.1642, grad=0.0000)]]\n",
      " Biases: [Var(v=-0.0201, grad=2.0140), Var(v=-0.0348, grad=3.4809)]\n",
      " Recurrent: Weights: [[Var(v=-0.0882, grad=2.9880), Var(v=0.1066, grad=1.7833)], [Var(v=-0.1436, grad=0.4929), Var(v=-0.0182, grad=0.2308)], [Var(v=-0.1524, grad=0.0000), Var(v=0.1099, grad=0.0000)], [Var(v=0.0207, grad=0.0000), Var(v=-0.3126, grad=0.0000)]]\n",
      " Initial hidden: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=-0.1084, grad=-0.1383), Var(v=-0.0520, grad=0.0400), Var(v=-0.0743, grad=0.1767), Var(v=0.0493, grad=-0.0004)], [Var(v=-0.0261, grad=0.3285), Var(v=-0.2326, grad=0.7229), Var(v=-0.0429, grad=0.4053), Var(v=-0.0834, grad=-0.0063)]]\n",
      " Biases: [Var(v=0.0795, grad=-7.9541), Var(v=0.1366, grad=-13.6572), Var(v=0.0593, grad=-5.9340), Var(v=-0.0012, grad=0.1196)]\n",
      "\n",
      "Network after zeroing gradients:\n",
      "Layer 0 \n",
      " \n",
      " Feed-forward: Weights: [[Var(v=-0.1284, grad=0.0000), Var(v=-0.0185, grad=0.0000)], [Var(v=0.0744, grad=0.0000), Var(v=-0.0485, grad=0.0000)], [Var(v=0.1754, grad=0.0000), Var(v=0.0701, grad=0.0000)], [Var(v=0.0953, grad=0.0000), Var(v=-0.1642, grad=0.0000)]]\n",
      " Biases: [Var(v=-0.0201, grad=0.0000), Var(v=-0.0348, grad=0.0000)]\n",
      " Recurrent: Weights: [[Var(v=-0.0882, grad=0.0000), Var(v=0.1066, grad=0.0000)], [Var(v=-0.1436, grad=0.0000), Var(v=-0.0182, grad=0.0000)], [Var(v=-0.1524, grad=0.0000), Var(v=0.1099, grad=0.0000)], [Var(v=0.0207, grad=0.0000), Var(v=-0.3126, grad=0.0000)]]\n",
      " Initial hidden: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=-0.1084, grad=0.0000), Var(v=-0.0520, grad=0.0000), Var(v=-0.0743, grad=0.0000), Var(v=0.0493, grad=0.0000)], [Var(v=-0.0261, grad=0.0000), Var(v=-0.2326, grad=0.0000), Var(v=-0.0429, grad=0.0000), Var(v=-0.0834, grad=0.0000)]]\n",
      " Biases: [Var(v=0.0795, grad=0.0000), Var(v=0.1366, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=-0.0012, grad=0.0000)]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {
    "id": "bYpEnbeMP4yL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backpropagation through time \r\n",
    "\r\n",
    "Since we have automatic differentiation we don't have to code the backpropagation rule by hand. Just to give you a bit of appreciation for have much bookkeeping is necessary we have given the derivation belwo.\r\n",
    "\r\n",
    "We need to compute the partial derivatives\r\n",
    "$\r\n",
    "\\frac{\\partial E}{\\partial W},~\\frac{\\partial E}{\\partial U},~\\frac{\\partial E}{\\partial V}\r\n",
    "$. \r\n",
    "We repeat the definition of the RNN forward pass from above:\r\n",
    "\r\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\r\n",
    "- $o_t = W\\,{h_t}$\r\n",
    "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\r\n",
    "\r\n",
    "where\r\n",
    "- $U$ is a weight matrix applied to the given input sample,\r\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\r\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output), and\r\n",
    "- $h$ is the hidden state (the network's memory) for a given time step.\r\n",
    "\r\n",
    "Recall though, that RNNs are recurrent and the weights $W,~U,~V$ are shared across time, i.e. we do not have separate weights for each time step. Therefore, to compute e.g. the partial derivative $\\frac{\\partial E}{\\partial W}$, we need to 1) sum up across time, and 2) apply the chain rule:\r\n",
    "\r\n",
    "$$\\frac{\\partial E}{\\partial W} = \\sum_{t} \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_{t}}{\\partial W}\\,.$$\r\n",
    "To compute $\\frac{\\partial o_{t}}{\\partial W}$ we use the definition of $o_t$ above.\r\n",
    "From week 1 (exercise i) we have that\r\n",
    "$$\\delta_{o,t} \\equiv \\frac{\\partial E}{\\partial o_{t}} = \\frac{\\partial E_t}{\\partial o_{t}} = \\hat{y}_{t} - y_{t}\\,,$$\r\n",
    "where $\\hat{y}_{t}$ is a softmax distribution over model outputs $o_{t}$ at time $t$, and $y_{t}$ is the target label at time $t$. \r\n",
    "\r\n",
    "To compute $\\frac{\\partial E}{\\partial U}$ and $\\frac{\\partial E}{\\partial V}$ we again sum over time and use the chain rule:\r\n",
    "$$\r\n",
    "\\frac{\\partial E}{\\partial U} = \\sum_{t} \\frac{\\partial E}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial U} \\ . \r\n",
    "$$\r\n",
    "This leads us to introduce\r\n",
    "$$\r\n",
    "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} \\ .\r\n",
    "$$\r\n",
    "The backpropagation through time recursion is derived by realising that a variation of $h_t$ affects 1) the loss at time step $t$ through the feed-forward connection to the output and 2) the future losses through the $h_{t+1}$ dependence of $h_t$. Mathematically, we write this through the chain rule:\r\n",
    "\r\n",
    "$$\r\n",
    "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} =  \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_t}{\\partial h_{t}} + \\frac{\\partial E}{\\partial h_{t+1}}\r\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} = \\delta_{o,t} \\frac{\\partial o_t}{\\partial h_{t}} + \\delta_{h,t+1}\r\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} \\ . \r\n",
    "$$\r\n",
    "\r\n",
    "Like above we can compute $\\frac{\\partial h_{t+1}}{\\partial h_{t}}$ using the definition of the network (shifted one time step). In the code the intermediate steps to compute the $\\delta$ recursions have been precomputed for you. \r\n",
    "\r\n",
    "For more information on backpropagation through time see the [Deep learning book section 10.2.2](https://www.deeplearningbook.org/contents/rnn.html).\r\n"
   ],
   "metadata": {
    "id": "ezSRiVJzk2h5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise f) Complete the training loop\r\n",
    "\r\n",
    "Complete the training loop above and run the training. You can leave the hyper-parameters and network size unchanged.\r\n",
    "\r\n",
    "Note that despite the small size of the network and dataset, training still takes quite a while. This is an issue with the recurrent structure of Nanograd. Using PyTorch, we would be able to use much larger datasets and models. We will attempt that in the bottom of the notebook. For now, you should get a feel of the recurrent structure of the RNN under the hood."
   ],
   "metadata": {
    "id": "XIy3OZaQSrVL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Initialize training hyperparameters\r\n",
    "EPOCHS = 200\r\n",
    "LR = 1e-2 \r\n",
    "LR_DECAY = 0.995"
   ],
   "outputs": [],
   "metadata": {
    "id": "MkaqbWmroncY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loss = []\r\n",
    "val_loss = []\r\n",
    "\r\n",
    "batch_size = 8\r\n",
    "\r\n",
    "for e in range(EPOCHS):\r\n",
    "    for b in range(int(np.ceil(len(encoded_training_set_x)/batch_size))):\r\n",
    "        # Forward pass and loss computation\r\n",
    "        Loss = \r\n",
    "\r\n",
    "        # Backward pass\r\n",
    "        Loss.backward()\r\n",
    "        \r\n",
    "        # gradient descent update\r\n",
    "        update_parameters(parameters(NN), LR)\r\n",
    "        zero_gradients(parameters(NN))\r\n",
    "      \r\n",
    "    LR = LR * LR_DECAY\r\n",
    "\r\n",
    "    # Training loss\r\n",
    "    Loss = \r\n",
    "    train_loss.append(Loss.v)\r\n",
    "        \r\n",
    "    # Validation loss\r\n",
    "    Loss_validation = \r\n",
    "    val_loss.append(Loss_validation.v)\r\n",
    "    \r\n",
    "    if e%5==0:\r\n",
    "        print(\"{:4d}\".format(e),\r\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \r\n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\r\n",
    "        \r\n",
    "# Plot training and validation loss\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "epoch = np.arange(len(train_loss))\r\n",
    "plt.figure()\r\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\r\n",
    "plt.plot(epoch, val_loss, 'b', label='Validation loss')\r\n",
    "plt.legend()\r\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "-JtM_IQjonfK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get first sentence in test set\r\n",
    "inputs, targets = test_set[0]\r\n",
    "\r\n",
    "# One-hot encode input and target sequence\r\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\r\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\r\n",
    "\r\n",
    "# Forward pass\r\n",
    "outputs = forward_batch(encoded_test_set_x[:1], NN)\r\n",
    "\r\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]\r\n",
    "\r\n",
    "print('Input sentence:')\r\n",
    "print(inputs)\r\n",
    "\r\n",
    "print('\\nTarget sequence:')\r\n",
    "print(targets)\r\n",
    "\r\n",
    "print('\\nPredicted sequence:')\r\n",
    "print([idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])])"
   ],
   "outputs": [],
   "metadata": {
    "id": "nAI_D6g25pTQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise g) Extrapolation\n",
    "\n",
    "Now that we have trained an RNN, it's time to put it to test. We will provide the network with a starting sentence and let it `freestyle` from there!\n",
    "\n",
    "How well does your RNN extrapolate -- does it work as expected? Are there any imperfections? If yes, why could that be?"
   ],
   "metadata": {
    "id": "Nn7QpUZXk2iH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def freestyle(NN, sentence='', num_generate=10):\r\n",
    "    \"\"\"\r\n",
    "    Takes in a sentence as a string and outputs a sequence\r\n",
    "    based on the predictions of the RNN.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "     `params`: the parameters of the network\r\n",
    "     `sentence`: string with whitespace-separated tokens\r\n",
    "     `num_generate`: the number of tokens to generate\r\n",
    "    \"\"\"\r\n",
    "    sentence = sentence.split(' ')\r\n",
    "    output_sentence = sentence\r\n",
    "    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)\r\n",
    "\r\n",
    "    # Begin predicting\r\n",
    "    outputs = forward_batch([sentence_one_hot], NN, use_stored_hid=False)\r\n",
    "    output_words = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]\r\n",
    "    word = output_words[-1]\r\n",
    "\r\n",
    "    # Append first prediction\r\n",
    "    output_sentence.append(word)\r\n",
    "\r\n",
    "    # Forward pass - Insert code here!\r\n",
    "    if word != 'EOS':\r\n",
    "      for i in range(num_generate-1):\r\n",
    "          sentence_one_hot = \r\n",
    "          outputs = \r\n",
    "          output_words = \r\n",
    "          word = \r\n",
    "          output_sentence.append(word)\r\n",
    "          if word == 'EOS':\r\n",
    "              break\r\n",
    "          \r\n",
    "    return output_sentence\r\n",
    "\r\n",
    "\r\n",
    "# Perform freestyle (extrapolation)\r\n",
    "test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n']\r\n",
    "for i, test_example in enumerate(test_examples):\r\n",
    "    print(f'Example {i}:', test_example)\r\n",
    "    print('Predicted sequence:', freestyle(NN, sentence=test_example), end='\\n\\n')"
   ],
   "outputs": [],
   "metadata": {
    "id": "4GNsD6HEJ-Gn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to the Long Short-Term Memory (LSTM) Cell\n",
    "\n",
    "Reading material: [Christopher Olah's walk-through](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "A vanilla RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the Long Short-Term Memory (LSTM) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. In this exercise, we will focus on LSTM but you would easily be able to go ahead and implement the GRU as well based on the principles that you learn here.\n",
    "\n",
    "Below is a figure of the LSTM cell:"
   ],
   "metadata": {
    "id": "X44hQ653vNCj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![lstm](https://i.imgur.com/3VkmUCe.png)\n",
    "Source: https://arxiv.org/abs/1412.7828"
   ],
   "metadata": {
    "id": "5Rgc-g3zwV9f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The LSTM cell contains three gates, input, forget, output gates and a memory cell.\n",
    "The output of the LSTM unit is computed with the following functions, where $\\sigma = \\mathrm{sigmoid}$.\n",
    "We have input gate $i$, forget gate $f$, and output gate $o$ defines as\n",
    "\n",
    "- $i = \\sigma ( W^i [h_{t-1}, x_t])$\n",
    "\n",
    "- $f = \\sigma ( W^f [h_{t-1},x_t])$\n",
    "\n",
    "- $o = \\sigma ( W^o [h_{t-1},x_t])$\n",
    "\n",
    "where $W^i, W^f, W^o$ are weight matrices applied to a concatenated $h_{t-1}$ (hidden state vector) and $x_t$ (input vector)  for each respective gate.\n",
    "\n",
    "$h_{t-1}$, from the previous time step along with the current input $x_t$ are used to compute the a candidate $g$\n",
    "\n",
    "- $g = \\mathrm{tanh}( W^g [h_{t-1}, x_t])$\n",
    "\n",
    "The value of the cell's memory, $c_t$, is updated as\n",
    "\n",
    "- $c_t = c_{t-1} \\circ f + g \\circ i$\n",
    "\n",
    "where $c_{t-1}$ is the previous memory, and $\\circ$ refers to element-wise multiplication (hint: element-wise multiplication is computed with the `*` operator in numpy).\n",
    "\n",
    "The output, $h_t$, is computed as\n",
    "\n",
    "- $h_t = \\mathrm{tanh}(c_t) \\circ o$\n",
    "\n",
    "and it is used for both the timestep's output and the next timestep, whereas $c_t$ is exclusively sent to the next timestep.\n",
    "This makes $c_t$ a memory feature, and is not used directly to compute the output of the timestep."
   ],
   "metadata": {
    "id": "ytasZ5cqw4W1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise h) Make the LSTMLayer class\n",
    "\n",
    "Make the LSTM class."
   ],
   "metadata": {
    "id": "m8_4RWp3k2iQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Insert code here\r\n",
    "\r\n",
    "class LSTMLayer:\r\n",
    "    def __init__(self, n_in: int, n_hid: int, act_fn, initializer = NormalInitializer(), initializer_hid = NormalInitializer()):\r\n",
    "        self.n_in = n_in\r\n",
    "        self.n_hid = n_hid\r\n",
    "        self.in_hid_layer = \r\n",
    "        self.g_layer = \r\n",
    "        self.i_layer = \r\n",
    "        self.f_layer = \r\n",
    "        self.o_layer = \r\n",
    "        self.initial_hid = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.stored_hid = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.initial_c = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.stored_c = [Var(0.0) for _ in range(n_hid)]\r\n",
    "        self.act_fn = act_fn\r\n",
    "    \r\n",
    "    def __repr__(self):    \r\n",
    "        return 'Feed-forward: ' + repr(self.in_hid_layer) + ' Candidate: ' + repr(self.g_layer) + ' i gate ' + repr(self.i_layer) + ' f gate ' + repr(self.f_layer) + ' o gate ' + repr(self.o_layer) + ' Initial hidden: ' + repr(self.initial_hid)\r\n",
    "\r\n",
    "    def parameters(self) -> Sequence[Var]:      \r\n",
    "      return self.in_hid_layer.parameters() + self.g_layer.parameters() + self.i_layer.parameters() + self.f_layer.parameters() + self.o_layer.parameters() + self.initial_hid\r\n",
    "\r\n",
    "    def forward_step(self, input: Sequence[Var], input_hid: Sequence[Var], input_c: Sequence[Var]) -> Sequence[Var]:\r\n",
    "        hids = []\r\n",
    "        cs = []\r\n",
    "        concatenated_input = []\r\n",
    "        for val in input_hid:\r\n",
    "          concatenated_input.append(val)\r\n",
    "        for val in input:\r\n",
    "          concatenated_input.append(val)\r\n",
    "\r\n",
    "        # Insert code here\r\n",
    "\r\n",
    "        return hids, cs\r\n",
    "    \r\n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]], use_stored_hid = False) -> Sequence[Sequence[Var]]:\r\n",
    "        out = []\r\n",
    "        if use_stored_hid:\r\n",
    "            hid = self.stored_hid\r\n",
    "            c = self.stored_c\r\n",
    "        else:\r\n",
    "            hid = self.initial_hid\r\n",
    "            c = self.initial_c\r\n",
    "        # Takes a sequence and loops over each character in the sequence. Note that each character has dimenson equal to the embeddng dimenson\r\n",
    "        for i in range(len(input)):\r\n",
    "            hid, c = # insert code here\r\n",
    "            out.append(hid)\r\n",
    "        self.stored_hid = hid\r\n",
    "        self.stored_c = c\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {
    "id": "qdU0yMXQU7d0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a bit of code to test it out:"
   ],
   "metadata": {
    "id": "gKu-bfhzk2iY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN = [\r\n",
    "    LSTMLayer(1, 5, lambda x: x.tanh()),\r\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\r\n",
    "]\r\n",
    "\r\n",
    "print(NN[0])\r\n",
    "x_train =[[[Var(1.0)], [Var(2.0)], [Var(3.0)]],\r\n",
    "          [[Var(1.0)], [Var(2.0)], [Var(3.0)]]]\r\n",
    "\r\n",
    "output_train = forward_batch(x_train, NN)          \r\n",
    "output_train[0][0][0].backward()\r\n",
    "\r\n",
    "print(output_train)"
   ],
   "outputs": [],
   "metadata": {
    "id": "u4AYroqSVRSv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise i) LSTM training\n",
    "\n",
    "Complete the LSTM training loop\n",
    "\n",
    "Run the training loop. Training time in Nanograd will likely be long, but see if you can find settings to compare your LSTM learning curve (NLL and number of epochs) to the vanilla RNN from earlier. Do you observe any improvements? Motivate your answer.\n",
    "\n",
    "Finally, below we will implement LSTM in PyTorch. You will notice it is much, much faster!"
   ],
   "metadata": {
    "id": "z4r4mgFsk2ik"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Initialize training hyperparameters\r\n",
    "EPOCHS = \r\n",
    "LR = \r\n",
    "LR_DECAY = "
   ],
   "outputs": [],
   "metadata": {
    "id": "MOAmppJD66tJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN = [\r\n",
    "    LSTMLayer(4, 1, lambda x: x.tanh()),\r\n",
    "    DenseLayer(1, 4, lambda x: x.identity())\r\n",
    "]\r\n",
    "\r\n",
    "train_loss = []\r\n",
    "val_loss = []\r\n",
    "\r\n",
    "batch_size = 8\r\n",
    "\r\n",
    "for e in range(EPOCHS):\r\n",
    "    for b in range(int(np.ceil(len(encoded_training_set_x)/batch_size))):\r\n",
    "        # Forward pass and loss computation\r\n",
    "        Loss =\r\n",
    "        # Backward pass\r\n",
    "        Loss.backward()\r\n",
    "        \r\n",
    "        # gradient descent update\r\n",
    "        update_parameters(parameters(NN), LR)\r\n",
    "        zero_gradients(parameters(NN))\r\n",
    "      \r\n",
    "    LR = LR * LR_DECAY\r\n",
    "\r\n",
    "    # Training loss\r\n",
    "    Loss = \r\n",
    "    train_loss.append(Loss.v)\r\n",
    "        \r\n",
    "    # Validation loss\r\n",
    "    Loss_validation = \r\n",
    "    val_loss.append(Loss_validation.v)\r\n",
    "    \r\n",
    "    if e%5==0:\r\n",
    "        print(\"{:4d}\".format(e),\r\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \r\n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\r\n",
    "        \r\n",
    "# Plot training and validation loss\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "epoch = np.arange(len(train_loss))\r\n",
    "plt.figure()\r\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\r\n",
    "plt.plot(epoch, val_loss, 'b', label='Validation loss')\r\n",
    "plt.legend()\r\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "tiotu2ab66w-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch implementation of the LSTM\n",
    "\n",
    "Now that we know how the LSTM cell works, let's see how easy it is to use in PyTorch!"
   ],
   "metadata": {
    "id": "gi51eWgKxyOk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of our LSTM network. We define a LSTM layer using the [nn.LSTM](https://pytorch.org/docs/stable/nn.html#lstm) class. The LSTM layer takes as argument the size of the input and the size of the hidden state like in our Nanograd implementation."
   ],
   "metadata": {
    "id": "O6HDdJLuk2ip"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "class MyRecurrentNet(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyRecurrentNet, self).__init__()\r\n",
    "        \r\n",
    "        # Recurrent layer\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        self.lstm = \r\n",
    "        \r\n",
    "        # Output layer\r\n",
    "        self.l_out = nn.Linear(in_features=50,\r\n",
    "                            out_features=vocab_size,\r\n",
    "                            bias=False)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        # RNN returns output and last hidden state\r\n",
    "        x, (h, c) = self.lstm(x)\r\n",
    "        \r\n",
    "        # Flatten output for feed-forward layer\r\n",
    "        x = x.view(-1, self.lstm.hidden_size)\r\n",
    "        \r\n",
    "        # Output layer\r\n",
    "        x = self.l_out(x)\r\n",
    "        \r\n",
    "        return x\r\n",
    "\r\n",
    "net = MyRecurrentNet()\r\n",
    "print(net)"
   ],
   "outputs": [],
   "metadata": {
    "id": "8UGrvknfk2ip"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise j) Train in PyTorch\n",
    "\n",
    "Define an LSTM for our recurrent neural network `MyRecurrentNet` above. A single LSTM layer is sufficient. What should the input size and hidden size be? Hint: use the PyTorch documentation.\n",
    "\n",
    "It's time for us to train our network. In the section below, you will get to put your deep learning skills to use and create your own training loop. You may want to consult previous exercises if you cannot recall how to define the training loop."
   ],
   "metadata": {
    "id": "J6r3bPwYk2is"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hyper-parameters\r\n",
    "num_epochs = 200\r\n",
    "\r\n",
    "# Initialize a new network\r\n",
    "net = MyRecurrentNet()\r\n",
    "\r\n",
    "# Define a loss function and optimizer for this problem\r\n",
    "# YOUR CODE HERE!\r\n",
    "criterion = \r\n",
    "optimizer = \r\n",
    "\r\n",
    "# Track loss\r\n",
    "training_loss, validation_loss = [], []\r\n",
    "\r\n",
    "# For each epoch\r\n",
    "for i in range(num_epochs):\r\n",
    "    \r\n",
    "    # Track loss\r\n",
    "    epoch_training_loss = 0\r\n",
    "    epoch_validation_loss = 0\r\n",
    "    \r\n",
    "    net.eval()\r\n",
    "        \r\n",
    "    # For each sentence in validation set\r\n",
    "    for inputs, targets in validation_set:\r\n",
    "        \r\n",
    "        # One-hot encode input and target sequence\r\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\r\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\r\n",
    "        \r\n",
    "        # Convert input to tensor\r\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\r\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\r\n",
    "        \r\n",
    "        # Convert target to tensor\r\n",
    "        targets_idx = torch.LongTensor(targets_idx)\r\n",
    "        \r\n",
    "        # Forward pass\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        outputs = \r\n",
    "        \r\n",
    "        # Compute loss\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        loss = \r\n",
    "        \r\n",
    "        # Update loss\r\n",
    "        epoch_validation_loss += loss.detach().numpy()\r\n",
    "    \r\n",
    "    net.train()\r\n",
    "    \r\n",
    "    # For each sentence in training set\r\n",
    "    for inputs, targets in training_set:\r\n",
    "        \r\n",
    "        # One-hot encode input and target sequence\r\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\r\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\r\n",
    "        \r\n",
    "        # Convert input to tensor\r\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\r\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\r\n",
    "        \r\n",
    "        # Convert target to tensor\r\n",
    "        targets_idx = torch.LongTensor(targets_idx)\r\n",
    "        \r\n",
    "        # Forward pass\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        outputs = \r\n",
    "        \r\n",
    "        # Compute loss\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        loss = \r\n",
    "        \r\n",
    "        # Backward pass\r\n",
    "        # YOUR CODE HERE!\r\n",
    "        # zero grad, backward, step...\r\n",
    "        \r\n",
    "        # Update loss\r\n",
    "        epoch_training_loss += loss.detach().numpy()\r\n",
    "        \r\n",
    "    # Save loss for plot\r\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\r\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\r\n",
    "\r\n",
    "    # Print loss every 10 epochs\r\n",
    "    if i % 10 == 0:\r\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\r\n",
    "\r\n",
    "        \r\n",
    "# Get first sentence in test set\r\n",
    "inputs, targets = test_set[1]\r\n",
    "\r\n",
    "# One-hot encode input and target sequence\r\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\r\n",
    "targets_idx = [word_to_idx[word] for word in targets]\r\n",
    "\r\n",
    "# Convert input to tensor\r\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\r\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\r\n",
    "\r\n",
    "# Convert target to tensor\r\n",
    "targets_idx = torch.LongTensor(targets_idx)\r\n",
    "\r\n",
    "# Forward pass\r\n",
    "outputs = net.forward(inputs_one_hot).data.numpy()\r\n",
    "\r\n",
    "print('\\nInput sequence:')\r\n",
    "print(inputs)\r\n",
    "\r\n",
    "print('\\nTarget sequence:')\r\n",
    "print(targets)\r\n",
    "\r\n",
    "print('\\nPredicted sequence:')\r\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\r\n",
    "\r\n",
    "# Plot training and validation loss\r\n",
    "epoch = np.arange(len(training_loss))\r\n",
    "plt.figure()\r\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\r\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\r\n",
    "plt.legend()\r\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "2URKsyFDx8xG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise k) Compare PyTorch and Nanograd implementations\n",
    "\n",
    "Compare the two implementations (in terms of predictive performance, training speed, etc.). Are they similar? How do they differ?\n",
    "\n",
    "\n",
    "Try to play around with the choice of hyper-parameters, optimizer, and hidden dimensions. How much can you improve the negative log-likelihood by these simple changes?"
   ],
   "metadata": {
    "id": "ydr7Czg_k2iw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise l) Other RNN cells (optional)\n",
    "\n",
    "Aside from the LSTM cell, various other RNN cells exist. The gated recurrent unit (GRU) is a variation of the LSTM cell that uses less gating mechanisms. Try to look it up in the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#gru) and switch out the LSTM cell in the code above. What do you notice in terms of performance and convergence speed?"
   ],
   "metadata": {
    "id": "M93ORx95k2ix"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise m) More complex tasks (optional)\n",
    "\n",
    "Go back and generate a more complex patterned dataset to learn from. Do you see any significant differences between a vanilla RNN and LSTM (implemented in e.g. PyTorch) when you increase the difficulty of the task?"
   ],
   "metadata": {
    "id": "juN400Ekk2iz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# It works, now what?"
   ],
   "metadata": {
    "id": "v68YEkEBk2iz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook you have learned how to use embeddings, recurrent neural networks, and the LSTM cell in particular.\n",
    "\n",
    "As we have already seen, RNNs are excellent for sequential data such as language. But what do we do if we're modelling data with strong dependency in both directions? Like in many things deep learning, we can build powerful models by stacking layers on top of each other; *bi-directional* RNNs consist of two LSTM cells, one for each direction. A sequence is first fed into the forward LSTM cell and the reversed sequence is then used as input to the backward LSTM cell together with the last hidden state from the forward LSTM cell. Follow [this link](https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf) for the original paper from 1997(!).\n",
    "\n",
    "For even deeper representations, multiple layers of both uni-directional and bi-directional RNNs can be stacked ontop of each other, just like feed-forward and convolutional layers. For more information on this, check out the [LSTM PyTorch documentation](https://pytorch.org/docs/stable/nn.html#lstm). Next week we will also explore ways to combine RNNs with other types of layers for even more expressive function approximators."
   ],
   "metadata": {
    "id": "NjpqSrSuk2i0"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bdA4LPsFiACe",
    "cGSoDRgHk2g1",
    "Dzmryk72k2g-",
    "M93ORx95k2ix"
   ],
   "name": "5.1-EXE-Recurrent-Neural-Networks-Nanograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('komp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "ed78582efbdb1cd8f8bfb63027333d779fdc8970c24bd6cce674fc83c0126038"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}